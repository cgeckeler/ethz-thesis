

% \subsection{RGB Image Reconstruction}
% We reconstruct color images by individually projecting only red, green, and blue light and capturing their images separately.
% To evaluate reconstruction accuracy, we compare our captures of the colored blocks on a camera calibration chart to the theoretical ground-truth values of the chart.
% Table \ref{table:Color_Error} shows the RMSE over all 16 color blocks for different image correction techniques. For the corrections, we use the information from the grayscale blocks on the chart (Fig. \ref{fig:chart_brightness_curves}).
% After proper white-balancing and linearization of the images, our method shows significant improvements in color accuracy. This can be traced mainly to its higher dynamic range (Fig. \ref{fig:chart_brightness_curves}).

% \begin{table}
% \centering
% \input{resources/tables/color_chart_rmse}
% \caption[Test Chart Color Errors]{Mean error between captured color values and their ground-truth on the test chart. Differences are computed using the euclidean distance in CIE 1976 L*a*b* color space. 'wb' is linearly white balanced, 'curve' corrects for the exponential scaling of measurements.}
% \label{table:Color_Error}
% \end{table}

% \include{resources/figures/calibration_brightness_plots}


% \begin{figure}
%     \centering
%     % \imageTable{resources/plots}{chart}{/}{bias_on_correction, bias_pr_correction, count_correction}{}{Bias (on), Bias (pr), Count}{4}{4}
%     \caption[Mappings from Sensor Measurements to Linear Light Intensity]{Visualization of the calibration procedure using the test chart. Points show the relationship between captured values and their actual brightness. The lines are the fitted exponential function for each color channel.}
% \label{fig:chart_brightness_curves}
% \end{figure}
% \subsection{Image Projection}
% To test the image reconstruction on complex scenes, we project and capture RGB images from the MS-COCO dataset\cite{MSCOCO}, for which there are ground-truth references available in the form of the original images (Fig. \ref{fig:COCOSetup}).
% This reverses our original method: Instead of projecting uniform light into a scene of varying reflectivity, we project images of changing brightness onto a uniformly reflective surface. This allows us to evaluate our method over a large dataset.
% The results show a clear quantitative improvement in reconstructed RGB image RMSE over the baseline method\cite{RGBD} by more than 24\%. This improvement can mainly be traced to two improvements: Our method achieves a better dynamic range than the baseline, allowing it to resolve bright and dark regions more accurately. Our method is also affected less by buffer overflow artefacts of the event camera because of its inherent redundancy and lower event-rate when capturing bright parts of the image.

% \begin{table}[]
%     \centering
%     \input{resources/tables/coco_rmse}
%     \caption[Image Capture Errors]{Root Mean Squared error between the captured images and their ground truth. Colors are in RGB color space.}
%     \label{fig:coco_results}
% \end{table}

% \include{resources/figures/annotated_coco_setup}
% \include{resources/figures/coco_results}


% \subsection{Multi-Spectral-Imaging}

% The area illuminated by the full spectrum light was very small in our setup (roughly a circle with 19 pixel radius). Therefore, we present multi-spectral measurements as the an average over the illuminated area. Segmentation tests were unfortunately not feasible with this setup.
% \include{resources/figures/multi_spectral_curves}


% \subsection{Depth Reconstruction}
% To measure the accuracy of the depth estimation, we compare depth measured across 8 scenes by 4 different sensors: The event camera, a Microsoft Kinect v2, pmd pico-flexx 2, and an Intel Realsense D435.
% In the case of the Realsense, we record its depth both with and without the aid of its laser projector. Similarly, we compute the depth from the event camera both directly from best matches and further optimized as in \cite{ESL}.
% The resulting point clouds are aligned to a ground-truth point cloud generated by a FARO Focus 3D S 120 laser scanner at a resolution of 58 points per degree horizontally and 33 points per degree vertically.
% To compare differences between scenes, the background and additional objects such as the desk lamp are removed manually.
% Table \ref{table:chamfer} shows the Chamfer distance between the reproduced point clouds and the ground-truth from the laser projector.
% The event camera shows a clear quantitative improvement by more than 30\% compared to the overall second best sensor - the Intel Realsense.

% \begin{table}
%     \centering
%     \input{resources/tables/depth_chamfer}
%     \caption[Point-Cloud Chamfer distance]{Mean Point-Cloud Chamfer distance in cm over 5 scenes. All scenes were recorded under overhead room illumination and in relative darkness with a close light source (HDR). The best and second best scores for each row are presented bold and underlined respectively.}
%     \label{table:chamfer}
% \end{table}


% \subsection{Semantic Segmentation}
% We evaluate the quality of our MD measurements by segmenting leaves from branches in cluttered scenes of a cherry laurel hedge.
% The segmentation is achieved using a three-step process. 
% \begin{enumerate}
%     \item Pre-cluster connected pixels based on their 3D point distances
%     \item Finely cluster using spectral clustering
%     \item Predict cluster classes using a neural network
% \end{enumerate}


%     \centering
%     \includegraphics[width=.8\textwidth]{resources/images/garden/ModelArchitecture.png}
%     \caption[Architecture of the Model used for Patch Classification]{The model combines two convolutional blocks with three fully connected layers. The input to the network is a 32x32 4-channel RGBD image. Its output is a logit for each of the two classes.}
%     \label{fig:model_architecture}
% \end{figure}
% \begin{figure}