\section{Conclusion}

This letter demonstrates an integrated event-based structured light system which can both generate high-resolution and low-latency depth reconstructions as well as integrated multispectral sensing. We demonstrate in the lab that by changing the projected wavelength of light used for structured light depth sensing, the event camera can act as a multispectral sensor. The results are comparable with conventional, commercial, off-the-shelf multispectral sensors. Finally, we showcase RGB color image reconstruction using a portable system limited to projecting RGB light, as well as material differentiation utilizing both depth and spectral data collected from a Masoala rainforest.

% Limitations
The current portable system only demonstrates multispectral sensing on RGB light, since these are the most commonly available commercial projectors. To have a true, portable multispectral sensing solution, that could also be mounted on a MAV, the correct wavelengths need to be projected. This is possible by either using a full-spectrum light source, with appropriate filters according to the desired wavelengths, or by having separate light sources per wavelength, such as multispectral LEDs. 
%future steps
Real-world tests demonstrated a major bottleneck with mobile light projectors coupled with event cameras; in order to be detectable, they need to overpower the ambient light. This requires a trade-off between light intensity and power efficiency, and for more reliable outdoor use in direct sunlight, the output power of the projected light should be adjusted. Lastly, the current system is still a prototype, for proper integration into a MAV-mountable platform, the system should be re-designed as a single, compact, sensing system.

% outlook
In this work we have demonstrated the feasibility of event spectroscopy. By using event-based structured light we can not only provide multispectral sensing of desired wavelengths, but also high-resolution and low-latency depth reconstruction as well as RGB color data. The utility is demonstrated on a downstream task of material differentiation of branches and leaves, an essential task for MAVs flying in forests. This system replaces and improves on the collected data from  multiple conventional imaging sensors currently needed for robot navigation and perception. This can help flying robots in the future access more environments, fly faster, safer, and collect more useful data. 