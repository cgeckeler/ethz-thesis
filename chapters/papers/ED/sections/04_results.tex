\section{Experiments}
This section evaluates the performance of our event-based depth and multispectral imaging system.
We begin by describing the hardware setup and the evaluation baselines and performance metrics.
We evaluate the performance of our system for depth estimation, comparing both qualitative and quantitative results to other depth sensors.
We then evaluate the performance of our system for multispectral imaging, comparing both qualitative and quantitative results to other multispectral imaging systems.
Finally, we evaluate the capability of our system for two tasks: material differentiation using full spectrum light source and material segmentation using  low spectrum projector and depth.
% \input{resources/figures/method_overview}
\begin{figure}
    \centering
    \includegraphics[width=.5\textwidth]{chapters/papers/ED/resources/images/multi-spectral/experiment_setup.png}
    \caption{Our full spectrum illumination setup.}
    \label{fig:generalSetup}
\end{figure}

\subsection{Hardware setup}
To the best of our knowledge, there exists no dataset on which the proposed approach can be evaluated.
We therefore, collect our own dataset by first building our prototype system using a Prophsee Gen3 event camera \cite{} and a projector.
% For depth reconstruction, we build on the work of \cite{ESL} and use a Prophsee Gen3 event camera \cite{} and Sony MP-CL1A projector.
% However, the projector can only project three distinct wavelengths (red, blur, green).
% To study the effect of multispectral imaging, we build a custom multispectral projector using rotating bandpass filter wheel and a full spectrum light source.
We provide more details of the hardware setup in the following sections.

\paragraph{Event camera}
In our setup, we use a Prophsee Gen3 event camera \cite{Posch11ssc} with a resolution of $640\times480$ pixels.
This sensor only provides regular events (change detection not exposure measurement) which are used for depth estimation and spectral imaging.
We use a lens with field of view of $60^\circ$.

\paragraph{Projector}
\input{chapters/papers/ED/resources/figures/test}
We use two different projectors for our experiments.
For depth estimation, we use a Sony MP-CL1A projector with a resolution of $1920\times720$ pixels.
However, this projector can only project three distinct wavelengths (red, blue, green).
Therefore, for multispectral imaging, we build a custom multispectral projector using a full spectrum light source and six wavelength bands of 10nm each.
% The full spectrum light source is a \mm{100W tungsten halogen lamp with a color temperature of 3200K}.
It is combined with an optical chopper which flickers the illumination at a frequency of $100Hz$.
\Fig \ref{fig:generalSetup} shows the setup of our multispectral projector.
As this illumination source is bulky, we cannot use it for outdoor and real world experiments.
Therefore, we use a Sony MP-CL1A projector for outdoor experiments which limits the spectral resolution to three distinct wavelengths.
% \include{resources/figures/annotated_spectral_setup}



\subsection{Evaluation}
We evaluate the performance of our system for depth estimation, multispectral imaging and material segmentation.

\subsubsection{Depth estimation}
\label{sec:eval:depth}
For depth estimation, we compare the performance of our system to existing depth sensors.
We compare our sensor against Microsoft Kinect V2 \cite{Kinectv2}, PMD Pico-Flexx 2\cite{pmdtec}, and an Intel Realsense D435 \cite{IntelRealSense}.
The groundtruth is collected using a FARO Focus 3D S 120 laser scanner.at a resolution of 
58 points per degree horizontally and 33 points per degree vertically.
The point clouds generated by these depth sensors are aligned to a ground-truth point cloud.
We compare the sensor point clouds and the ground-truth point cloud using the root-mean square error (RMSE), fill rate and Chamfer distance.
RMSE computes the distance for each point in the pointcloud to the closest neighbor in the ground-truth point-cloud.
Fill rate computes the percentage of points in the ground-truth point-cloud that have a corresponding point in the sensor point-cloud.
While the first metric shows how accurate existing measurements are, the second metric better captures how complete the coverage of measurements is

% \paragraph{Results}
% \mm{ToDo: add depth comparison with other sensors}\\
\input{chapters/papers/ED/resources/figures/depth_comparison_short}
\input{chapters/papers/ED/resources/tables/depth_chamfer}
% \mm{Add table for comparison for all objects}
% \mm{Supplementary: Hole and plane reconstruction}
We show qualitative results on depth reconstruction of all the depth sensors in \Fig \ref{fig:depth_comparison}.
The realsense depth camera uses active stereo depth estimation, which projects a sparse random dot pattern.
The sparse depth is filled using a hole-filling approach, resulting in overestimation of depth and missing the structural details.
Kinect, on the otherhand, uses time-of-flight based depth sensing,where each pixel has a pulsed light emitter and detector. 
This makes the pixel size much larger, therefore despite the densely illuminated scene, the depth fails to capture thin structure such as the wire frame (row $2$).
Moreover, the due to the inherent issues of time-of-flight sensors it cannot capture the depth of face of statue of David due to strong inter-reflection, shown in row $3$ 
Our method on the hand achieves the perfect balance by capturing the intricate details of the wireframe and the globe structure, due to its dense projected pattern.
Quantitative comparisons using the Chamfer distance for the standard illumination show that our event-based depth sensor is $\%$ better than the state-of-the-art depth sensor for the same framerate \Tab \ref{tab:depthTable}.
\paragraph{Effect of Illumination}
\input{chapters/papers/ED/resources/tables/depth_illumination}
Increasing the illumination results in overexposure of frames, further degrading the depth by $\%$ on average for state-of-the-art frame-based depth sensor \Tab \ref{tab:depth_illum}.
Our method, on the otherhand, has a negligible drop in performance at higher illumination.

\subsubsection{Multispectral imaging}
\label{sec:eval:color}
In this section, we evaluate the spectral imaging capability of our setup.
We compare our method against the event counting baseline proposed in \cite{RGBD}.
Note, that while our method is agnostic to the spectral illumination source used, practical constrains limited the usability of our full spectrum light source in natural scenes.
Therefore, we use our visible range projector for most of our experiments which only projects three distinct wavelengths.
Nevertheless, we show the application of our full spectrum illumination source for material differentiation.

\paragraph{Color accuracy}
% \input{resources/figures/test}
We compare the color accuracy on an ISO $1233:2017$ conforming chart.
The chart features $16$ color blocks which are distributed over the sRGB spectrum.
We measure the mean color for each block and compare it to the groundtruth using $L2$ distance in the CIE $1976$.
This CIE is based on human perception of colors and serves as a standard reference for understanding and quantifying color.
We reconstruct color images by individually projecting only red, green, and blue light and capturing their images separately.
Table \ref{table:Color_Error} shows the RMSE over all 16 color blocks for different image correction techniques. 
We use the information from the grayscale blocks on the chart (Fig. \ref{fig:chart_brightness_curves}) to apply correction.
After white-balancing and linearization of the images, our method shows significant improvements in color accuracy. 
This can be traced mainly to its higher dynamic range (Fig. \ref{fig:chart_brightness_curves}).
\input{chapters/papers/ED/resources/tables/color_chart_rmse}
% \include{resources/figures/calibration_brightness_plots}

\paragraph{Image reconstruction accuracy}
\input{chapters/papers/ED/resources/figures/coco_results}
% \mm{Add image reconstruction fig comparison}\\
% \nn{done}
We also evaluate our approach to reconstruct more complex images such as from the MSCOCO dataset \cite{MSCOCO}.
The results show a clear quantitative improvement in reconstructed RGB image RMSE over the baseline method\cite{RGBD} by more than 24\%. This improvement can mainly be traced to two improvements: 
(i) Our method achieves a better dynamic range than the baseline, allowing it to resolve bright and dark regions more accurately. 
(ii) We are less affected by buffer overflow artefacts of the event camera because of its inherent redundancy and lower event-rate when capturing bright parts of the image.

\begin{table}[]
    \centering
    \input{chapters/papers/ED/resources/tables/coco_rmse}
    \caption[Image Capture Errors]{Root Mean Squared error between the captured images and their ground truth. Colors are in RGB color space.}
    \label{fig:coco_results}
\end{table}
