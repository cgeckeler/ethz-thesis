\section{Related works}
\subsection{Spectral imaging}

Spectral imaging is an established field with applications in astronomy, precising farming, and many more.
As a result, there is a large range of methodologies with varying trade-offs that may fit one field better than another.
The goal of spectral imaging is to capture images of multiple wavelength bands resulting in an $(X,Y,W)$ data-cube.
Methods to achieve this are commonly divided into two major groups: scanning and snapshot.
Snapshot methods capture the full data-cube during a single integration period of the sensor whereas scanning methods achieve the same over multiple periods.
\newline
\newline
A popular realm of scanning methods are tuned filters. These can take the form of a rotating filter wheel, an electro-mechanical Fabry-Ferot filter\cite{MEMSFabryFerot} \cite{Atherton1981}, liquid-crystals \cite{Gupta2008}, or using an acousto-optic tunable filter \cite{poger2001multispectral}.
\newline
Multiplexing techniques such as Fourier Transform Spectroscopy\cite{} and Computed
Tomography Multi-Spectral-Imaging\cite{} allow for the reconstruction of multiple
wavelengths from fewer images than the number of wavelengths at the expense of
some artifacts.
\newline
\nn{TODO check and distribute citations}
Classic snapshot methods make use of multiple sensors in the form of Bayer Pattern filter layouts, beam-splitters, or simply by using multiple full camera sensors.
Recent developments in compressive sensing like CASSI use a two-dimensional patterned grating to reconstruct all images of all wavelengths in a single shot[31]. A major difficulty of this method has been the computational complexity required to decode the image, but improvements in computational power and improved algorithms have made this method viable for high-resolution imaging in both spatial and spectral \cite{Gehm2007} \cite{Zhang2011} \cite{Wang17PAMI}
\newline
\newline
In this work, we employ a variant of a tuned filter by illuminating the scene with filtered light of specific wavelength bands and measuring the resulting change in light reflected by the scene with respect to ambient illumination.

\subsection{Event-based radiance recovery}
Event-based radiance recovery is an evolving field of research, witnessing significant exploration.
As events naturally compress the visual information, estimating the absolute intensity from events is a challenging task.
% passive image reconstruction methods
Prior methods assume \cite{} or data-driven priors \cite{Rebecq2018,}, which results in grayscale image reconstruction upto an unknown intensity value.
Color intensity can be recovered by using color event camera \cite{DAVIScolor, cedric_color}.
In \cite{Berkelyevent2noise}, they proposed to use the illumination-dependent noise characteristics of event camera to reconstruct the intensity.
However, this method was primarily developed for static scenes where there is no relative motion between the camera and scene, rendering the high-temporal resolution of events meaningless.

% Integrating event data with RGB cameras has facilitated frame interpolation, leveraging events for temporal changes while relying on RGB cameras for color information. 
Recent progress of combining active illumination with event cameras \cite{Brandli13fns,Martel18icas, 
Matsuda15ICCP, ESL}, has resulted in accurate and high-speed geometry estimation.
The setup consists of a laser scanning projector which illuminates the scene with a known pattern and the reflection of this pattern observed by the event camera is used to triangulate the depth of the scene.
While it was shown in \cite{Matsuda15ICCP} that the events generated by this reflection are independent of the scene reflectivity, it is only true for an ideal sensor.
The second order noise characteristics of event camera, however, are illumination dependent thus creating non-idealities.
This principle was used in \cite{Ehsan2022} to reconstruct the absolute intensity by observing how many events were generated by the reflected light.
For darker objects, the event count would be lower as it reflects lower intensity of incoming light, while brighter materials reflect more light and therefore trigger more events.
In our work, we expand upon ESL's\cite{ESL} principles utilizing a projector for illumination. 
Extending its application beyond depth estimation, we employ the projector to extract spectral properties of scenes through collaboration with an event camera, broadening the scope of capturing and analyzing scene characteristics.
% \mm{ToOD add how we are better than baseline.}

% \subsection*{Event-based depth estimation}
% \subsection{Event-based radiance estimation}
% \begin{itemize}
%     \item Color event cameras: (disadvantage bayer pattern reduces spatial resolution, requires relative motion)
%     \item Dual-camera setups: combining color camera with event camera (beam splitter setup/dual camera setup, requires relative motion)
%     \item RGB reconstruction with SL (closest)
% \end{itemize}
% \subsection{Event-based depth estimation}
% Event cameras have shown significant advantages over frame-base counterparts for depth estimation in both active and passive illumination.
% In the presence of active illumination, event-based structured light systems.
% \begin{itemize}
%     \item EMVS, E2Depth
%     \item Event+SL
% \end{itemize}
